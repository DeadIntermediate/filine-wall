/**
 * Advanced Voice Pattern Analysis using TensorFlow.js
 * Real-time ML analysis of voice characteristics to detect robocalls and scammers
 */

// Conditional TensorFlow import - only load if ML features are enabled
let tf: any = null;
try {
  if (process.env.ENABLE_VOICE_ANALYSIS === 'true' || process.env.FEATURE_VOICE_ANALYSIS === 'true') {
    tf = require('@tensorflow/tfjs-node');
  }
} catch (error) {
  console.warn('âš  Voice analysis not available:', (error as Error).message);
}

import { AudioContext } from 'web-audio-api';
import { spawn } from 'child_process';
import { EventEmitter } from 'events';
import { logger } from '../utils/logger.js';

interface VoiceFeatures {
    // Fundamental frequency characteristics
    fundamentalFrequency: number[];
    pitch: number[];
    formants: number[];
    
    // Speech patterns
    speechRate: number;
    pausePatterns: number[];
    energyDistribution: number[];
    
    // Voice quality indicators
    jitter: number;
    shimmer: number;
    harmonicsToNoiseRatio: number;
    
    // Robocall indicators
    backgroundNoise: number;
    compressionArtifacts: number;
    digitalArtifacts: number;
}

interface VoiceAnalysisResult {
    isRobocall: boolean;
    confidence: number;
    riskScore: number;
    detectedFeatures: string[];
    reasoning: string;
}

export class AdvancedVoiceAnalyzer extends EventEmitter {
    private model: any = null;
    private audioContext: AudioContext;
    private isInitialized = false;
    private featureBuffer: Float32Array[] = [];
    private analysisWindow = 3; // seconds
    
    constructor() {
        super();
        this.audioContext = new AudioContext();
        this.initializeModel();
    }

    /**
     * Initialize the ML model for voice analysis
     */
    private async initializeModel(): Promise<void> {
        try {
            // Try to load pre-trained model, fallback to creating new one
            try {
                this.model = await tf.loadLayersModel('file://./models/voice-analysis-model.json');
                logger.info('Loaded existing voice analysis model');
            } catch (error) {
                logger.info('Creating new voice analysis model');
                await this.createVoiceAnalysisModel();
            }
            
            this.isInitialized = true;
            this.emit('ready');
        } catch (error) {
            logger.error('Failed to initialize voice analysis model:', error instanceof Error ? error : undefined);
            throw error;
        }
    }

    /**
     * Create a new neural network model for voice analysis
     */
    private async createVoiceAnalysisModel(): Promise<void> {
        // Multi-layer neural network for voice pattern recognition
        this.model = tf.sequential({
            layers: [
                // Input layer for voice features
                tf.layers.dense({
                    inputShape: [50], // 50 voice features
                    units: 128,
                    activation: 'relu',
                    kernelRegularizer: tf.regularizers.l2({ l2: 0.001 })
                }),
                
                // Dropout for regularization
                tf.layers.dropout({ rate: 0.3 }),
                
                // Hidden layers for pattern detection
                tf.layers.dense({
                    units: 64,
                    activation: 'relu',
                    kernelRegularizer: tf.regularizers.l2({ l2: 0.001 })
                }),
                
                tf.layers.dropout({ rate: 0.2 }),
                
                tf.layers.dense({
                    units: 32,
                    activation: 'relu'
                }),
                
                // Output layer for classification
                tf.layers.dense({
                    units: 3, // [human, robocall, scammer]
                    activation: 'softmax'
                })
            ]
        });

        // Compile with appropriate optimizer and loss function
        this.model.compile({
            optimizer: tf.train.adam(0.001),
            loss: 'categoricalCrossentropy',
            metrics: ['accuracy', 'precision', 'recall']
        });

        logger.info('Created new voice analysis neural network model');
    }

    /**
     * Analyze real-time voice data from modem
     */
    async analyzeVoiceStream(audioData: Buffer): Promise<VoiceAnalysisResult> {
        if (!this.isInitialized || !this.model) {
            throw new Error('Voice analyzer not initialized');
        }

        try {
            // Extract voice features from audio data
            const features = await this.extractVoiceFeatures(audioData);
            
            // Prepare features for ML model
            const featureTensor = tf.tensor2d([this.flattenFeatures(features)]);
            
            // Run inference
            const prediction = this.model.predict(featureTensor) as any;
            const probabilities = await prediction.data();
            
            // Interpret results
            const result = this.interpretPrediction(probabilities as Float32Array, features);
            
            // Cleanup tensors
            featureTensor.dispose();
            prediction.dispose();
            
            // Update model with feedback if available
            this.updateModelWithFeedback(features, result);
            
            return result;
            
        } catch (error) {
            logger.error('Voice analysis failed:', error instanceof Error ? error : undefined);
            throw error;
        }
    }

    /**
     * Extract comprehensive voice features from audio data
     */
    private async extractVoiceFeatures(audioData: Buffer): Promise<VoiceFeatures> {
        const features: VoiceFeatures = {
            fundamentalFrequency: [],
            pitch: [],
            formants: [],
            speechRate: 0,
            pausePatterns: [],
            energyDistribution: [],
            jitter: 0,
            shimmer: 0,
            harmonicsToNoiseRatio: 0,
            backgroundNoise: 0,
            compressionArtifacts: 0,
            digitalArtifacts: 0
        };

        try {
            // Convert audio buffer to analyzable format
            const audioFloat = this.bufferToFloat32Array(audioData);
            
            // Fundamental frequency analysis
            features.fundamentalFrequency = this.analyzeFundamentalFrequency(audioFloat);
            features.pitch = this.analyzePitchVariation(audioFloat);
            
            // Formant analysis for speech characteristics
            features.formants = this.analyzeFormants(audioFloat);
            
            // Speech timing analysis
            features.speechRate = this.analyzeSpeechRate(audioFloat);
            features.pausePatterns = this.analyzePausePatterns(audioFloat);
            
            // Energy distribution analysis
            features.energyDistribution = this.analyzeEnergyDistribution(audioFloat);
            
            // Voice quality metrics
            features.jitter = this.calculateJitter(audioFloat);
            features.shimmer = this.calculateShimmer(audioFloat);
            features.harmonicsToNoiseRatio = this.calculateHNR(audioFloat);
            
            // Digital artifact detection
            features.backgroundNoise = this.detectBackgroundNoise(audioFloat);
            features.compressionArtifacts = this.detectCompressionArtifacts(audioFloat);
            features.digitalArtifacts = this.detectDigitalArtifacts(audioFloat);
            
            return features;
            
        } catch (error) {
            logger.error('Feature extraction failed:', error instanceof Error ? error : undefined);
            return features; // Return partial features
        }
    }

    /**
     * Analyze fundamental frequency patterns typical of robocalls
     */
    private analyzeFundamentalFrequency(audio: Float32Array): number[] {
        const windowSize = 1024;
        const frequencies: number[] = [];
        
        for (let i = 0; i < audio.length - windowSize; i += windowSize / 2) {
            const window = audio.slice(i, i + windowSize);
            const fft = this.computeFFT(window);
            const fundamentalFreq = this.findFundamentalFrequency(fft);
            frequencies.push(fundamentalFreq);
        }
        
        return frequencies;
    }

    /**
     * Analyze pitch variation patterns (robocalls often have unnatural pitch)
     */
    private analyzePitchVariation(audio: Float32Array): number[] {
        const pitchValues: number[] = [];
        const windowSize = 2048;
        
        for (let i = 0; i < audio.length - windowSize; i += windowSize / 4) {
            const window = audio.slice(i, i + windowSize);
            const pitch = this.estimatePitch(window);
            pitchValues.push(pitch);
        }
        
        return pitchValues;
    }

    /**
     * Analyze formant frequencies (speech resonances)
     */
    private analyzeFormants(audio: Float32Array): number[] {
        const windowSize = 1024;
        const formants: number[] = [];
        
        for (let i = 0; i < audio.length - windowSize; i += windowSize) {
            const window = audio.slice(i, i + windowSize);
            const spectrum = this.computeFFT(window);
            const formantFreqs = this.extractFormants(spectrum);
            formants.push(...formantFreqs);
        }
        
        return formants;
    }

    /**
     * Analyze speech rate (robocalls often have consistent, unnatural timing)
     */
    private analyzeSpeechRate(audio: Float32Array): number {
        const energy = this.calculateEnergy(audio);
        const speechSegments = this.detectSpeechSegments(energy);
        
        if (speechSegments.length < 2) return 0;
        
        const totalSpeechTime = speechSegments.reduce((sum, segment) => 
            sum + (segment.end - segment.start), 0);
        const wordsPerSecond = speechSegments.length / (totalSpeechTime / 44100);
        
        return wordsPerSecond;
    }

    /**
     * Detect compression artifacts typical of robocalls
     */
    private detectCompressionArtifacts(audio: Float32Array): number {
        const spectrum = this.computeFFT(audio);
        let artifactScore = 0;
        
        // Look for compression-related frequency anomalies
        for (let i = 1; i < spectrum.length - 1; i++) {
            const current = spectrum[i];
            const prev = spectrum[i - 1];
            const next = spectrum[i + 1];
            
            if (!current || !prev || !next) continue;
            
            const currentMag = Math.sqrt(current.real ** 2 + current.imag ** 2);
            const prevMag = Math.sqrt(prev.real ** 2 + prev.imag ** 2);
            const nextMag = Math.sqrt(next.real ** 2 + next.imag ** 2);
            
            // Detect sudden drops/spikes typical of compression
            if (currentMag > prevMag * 2 && currentMag > nextMag * 2) {
                artifactScore += 1;
            }
        }
        
        return artifactScore / spectrum.length;
    }

    /**
     * Detect digital artifacts and processing signatures
     */
    private detectDigitalArtifacts(audio: Float32Array): number {
        let artifactScore = 0;
        
        // Check for quantization noise
        const quantizationNoise = this.detectQuantizationNoise(audio);
        artifactScore += quantizationNoise;
        
        // Check for aliasing
        const aliasingScore = this.detectAliasing(audio);
        artifactScore += aliasingScore;
        
        // Check for digital filters
        const filteringScore = this.detectDigitalFiltering(audio);
        artifactScore += filteringScore;
        
        return Math.min(artifactScore, 1.0);
    }

    /**
     * Calculate jitter (pitch period variation)
     */
    private calculateJitter(audio: Float32Array): number {
        const periods = this.extractPitchPeriods(audio);
        if (periods.length < 2) return 0;
        
        let jitterSum = 0;
        for (let i = 1; i < periods.length; i++) {
            const current = periods[i];
            const prev = periods[i - 1];
            if (current !== undefined && prev !== undefined) {
                jitterSum += Math.abs(current - prev);
            }
        }
        
        const meanPeriod = periods.reduce((sum, p) => sum + p, 0) / periods.length;
        return (jitterSum / (periods.length - 1)) / meanPeriod;
    }

    /**
     * Calculate shimmer (amplitude variation)
     */
    private calculateShimmer(audio: Float32Array): number {
        const amplitudes = this.extractAmplitudes(audio);
        if (amplitudes.length < 2) return 0;
        
        let shimmerSum = 0;
        for (let i = 1; i < amplitudes.length; i++) {
            const current = amplitudes[i];
            const prev = amplitudes[i - 1];
            if (current !== undefined && prev !== undefined) {
                shimmerSum += Math.abs(current - prev);
            }
        }
        
        const meanAmplitude = amplitudes.reduce((sum, a) => sum + a, 0) / amplitudes.length;
        return (shimmerSum / (amplitudes.length - 1)) / meanAmplitude;
    }

    /**
     * Flatten features for neural network input
     */
    private flattenFeatures(features: VoiceFeatures): number[] {
        const flattened: number[] = [];
        
        // Statistical measures of frequency features
        flattened.push(this.mean(features.fundamentalFrequency));
        flattened.push(this.std(features.fundamentalFrequency));
        flattened.push(this.mean(features.pitch));
        flattened.push(this.std(features.pitch));
        
        // Formant statistics
        flattened.push(this.mean(features.formants));
        flattened.push(this.std(features.formants));
        
        // Speech timing
        flattened.push(features.speechRate);
        flattened.push(this.mean(features.pausePatterns));
        
        // Energy distribution
        flattened.push(...features.energyDistribution.slice(0, 10)); // First 10 bins
        
        // Voice quality
        flattened.push(features.jitter);
        flattened.push(features.shimmer);
        flattened.push(features.harmonicsToNoiseRatio);
        
        // Digital artifacts
        flattened.push(features.backgroundNoise);
        flattened.push(features.compressionArtifacts);
        flattened.push(features.digitalArtifacts);
        
        // Pad or truncate to exactly 50 features
        while (flattened.length < 50) {
            flattened.push(0);
        }
        
        return flattened.slice(0, 50);
    }

    /**
     * Interpret ML model prediction results
     */
    private interpretPrediction(probabilities: Float32Array, features: VoiceFeatures): VoiceAnalysisResult {
        const [humanProb = 0, robocallProb = 0, scammerProb = 0] = Array.from(probabilities);
        
        const isRobocall = robocallProb > 0.5 || scammerProb > 0.3;
        const confidence = Math.max(humanProb, robocallProb, scammerProb);
        const riskScore = (robocallProb * 0.7) + (scammerProb * 1.0) + (1 - humanProb) * 0.3;
        
        const detectedFeatures: string[] = [];
        let reasoning = '';
        
        // Analyze specific features for explanation
        if (features.jitter > 0.02) {
            detectedFeatures.push('abnormal_pitch_variation');
            reasoning += 'Detected unnatural pitch variations typical of synthetic speech. ';
        }
        
        if (features.compressionArtifacts > 0.1) {
            detectedFeatures.push('compression_artifacts');
            reasoning += 'Audio shows signs of heavy compression used in robocalls. ';
        }
        
        if (features.speechRate < 2 || features.speechRate > 8) {
            detectedFeatures.push('abnormal_speech_rate');
            reasoning += 'Speech rate is outside normal human range. ';
        }
        
        if (features.backgroundNoise > 0.3) {
            detectedFeatures.push('suspicious_background');
            reasoning += 'Background noise pattern suggests call center environment. ';
        }
        
        if (robocallProb > 0.7) {
            reasoning += 'High probability of robocall based on voice synthesis patterns.';
        } else if (scammerProb > 0.5) {
            reasoning += 'Voice patterns consistent with known scammer characteristics.';
        }
        
        return {
            isRobocall,
            confidence,
            riskScore: Math.min(riskScore, 1.0),
            detectedFeatures,
            reasoning: reasoning.trim() || 'Analysis based on comprehensive voice pattern recognition.'
        };
    }

    /**
     * Update model with user feedback for continuous learning
     */
    private async updateModelWithFeedback(features: VoiceFeatures, result: VoiceAnalysisResult): Promise<void> {
        // Store for batch learning updates
        this.featureBuffer.push(new Float32Array(this.flattenFeatures(features)));
        
        // Periodic model updates with accumulated feedback
        if (this.featureBuffer.length >= 100) {
            await this.performBatchLearning();
            this.featureBuffer = [];
        }
    }

    /**
     * Perform batch learning with accumulated data
     */
    private async performBatchLearning(): Promise<void> {
        if (!this.model || this.featureBuffer.length === 0) return;
        
        try {
            // Convert buffer to tensors - use Array.from to convert Float32Array[] to number[][]
            const bufferAsArray = this.featureBuffer.map(arr => Array.from(arr));
            const xs = tf.tensor2d(bufferAsArray);
            
            // Generate pseudo-labels based on current model predictions
            const predictions = this.model.predict(xs) as any;
            
            // Semi-supervised learning update
            await this.model.fit(xs, predictions, {
                epochs: 1,
                verbose: 0,
                validationSplit: 0.2
            });
            
            xs.dispose();
            predictions.dispose();
            
            logger.info('Performed batch learning update with voice analysis model');
            
        } catch (error) {
            logger.error('Batch learning failed:', error instanceof Error ? error : undefined);
        }
    }

    // Utility methods for audio processing
    private bufferToFloat32Array(buffer: Buffer): Float32Array {
        const int16Array = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.length / 2);
        const float32Array = new Float32Array(int16Array.length);
        
        for (let i = 0; i < int16Array.length; i++) {
            const value = int16Array[i];
            if (value !== undefined) {
                float32Array[i] = value / 32768.0;
            }
        }
        
        return float32Array;
    }

    private computeFFT(audio: Float32Array): Complex[] {
        // Simplified FFT implementation - in production, use a proper FFT library
        const N = audio.length;
        const result: Complex[] = [];
        
        for (let k = 0; k < N; k++) {
            let real = 0;
            let imag = 0;
            
            for (let n = 0; n < N; n++) {
                const audioValue = audio[n];
                if (audioValue !== undefined) {
                    const angle = -2 * Math.PI * k * n / N;
                    real += audioValue * Math.cos(angle);
                    imag += audioValue * Math.sin(angle);
                }
            }
            
            result.push({ real, imag });
        }
        
        return result;
    }

    private mean(values: number[]): number {
        if (values.length === 0) return 0;
        return values.reduce((sum, val) => sum + val, 0) / values.length;
    }

    private std(values: number[]): number {
        if (values.length === 0) return 0;
        const avg = this.mean(values);
        const variance = values.reduce((sum, val) => sum + Math.pow(val - avg, 2), 0) / values.length;
        return Math.sqrt(variance);
    }

    // Placeholder methods for audio analysis (implement with proper DSP libraries)
    private findFundamentalFrequency(fft: Complex[]): number { return 0; }
    private estimatePitch(window: Float32Array): number { return 0; }
    private extractFormants(spectrum: Complex[]): number[] { return []; }
    private calculateEnergy(audio: Float32Array): number[] { return []; }
    private detectSpeechSegments(energy: number[]): Array<{start: number, end: number}> { return []; }
    private detectQuantizationNoise(audio: Float32Array): number { return 0; }
    private detectAliasing(audio: Float32Array): number { return 0; }
    private detectDigitalFiltering(audio: Float32Array): number { return 0; }
    private extractPitchPeriods(audio: Float32Array): number[] { return []; }
    private extractAmplitudes(audio: Float32Array): number[] { return []; }
    private analyzeEnergyDistribution(audio: Float32Array): number[] { return new Array(20).fill(0); }
    private analyzePausePatterns(audio: Float32Array): number[] { return []; }
    private calculateHNR(audio: Float32Array): number { return 0; }
    private detectBackgroundNoise(audio: Float32Array): number { return 0; }

    /**
     * Save the trained model
     */
    async saveModel(): Promise<void> {
        if (!this.model) throw new Error('No model to save');
        
        await this.model.save('file://./models/voice-analysis-model');
        logger.info('Voice analysis model saved');
    }

    /**
     * Get model performance metrics
     */
    getModelMetrics(): any {
        return {
            isInitialized: this.isInitialized,
            bufferSize: this.featureBuffer.length,
            analysisWindow: this.analysisWindow
        };
    }
}

interface Complex {
    real: number;
    imag: number;
}

export default AdvancedVoiceAnalyzer;